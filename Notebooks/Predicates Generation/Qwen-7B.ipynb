{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-02T14:00:33.851981Z",
     "iopub.status.busy": "2025-07-02T14:00:33.851701Z",
     "iopub.status.idle": "2025-07-02T14:01:38.361403Z",
     "shell.execute_reply": "2025-07-02T14:01:38.360720Z",
     "shell.execute_reply.started": "2025-07-02T14:00:33.851964Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T12:22:04.070305Z",
     "iopub.status.busy": "2025-07-02T12:22:04.069951Z",
     "iopub.status.idle": "2025-07-02T12:26:02.647625Z",
     "shell.execute_reply": "2025-07-02T12:26:02.646838Z",
     "shell.execute_reply.started": "2025-07-02T12:22:04.070286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T12:26:02.648972Z",
     "iopub.status.busy": "2025-07-02T12:26:02.648686Z",
     "iopub.status.idle": "2025-07-02T12:29:36.501986Z",
     "shell.execute_reply": "2025-07-02T12:29:36.501405Z",
     "shell.execute_reply.started": "2025-07-02T12:26:02.648938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",        # Latest Qwen2.5 VL\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# Load the vision model\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
    "    # \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    device_map=\"balanced\",  # Distribute across both T4 GPUs\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    # Aggressive memory limits for T4x2\n",
    "    max_memory={\n",
    "        0: \"13GB\",  # GPU 0 - leave some buffer\n",
    "        1: \"13GB\",  # GPU 1 - leave some buffer\n",
    "        \"cpu\": \"20GB\"\n",
    "    },\n",
    "    offload_folder=\"./offload_temp\",\n",
    "    offload_state_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T12:29:36.503612Z",
     "iopub.status.busy": "2025-07-02T12:29:36.502859Z",
     "iopub.status.idle": "2025-07-02T12:29:36.523606Z",
     "shell.execute_reply": "2025-07-02T12:29:36.522851Z",
     "shell.execute_reply.started": "2025-07-02T12:29:36.503589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enable for inference\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()  # set to eval mode\n",
    "\n",
    "def generate_predicates_from_image(image_path, question):\n",
    "    \"\"\"\n",
    "    Generate predicates/literals from geometry problem image using Qwen2.5-VL Model\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the geometry problem image\n",
    "        question (str): Question text\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'content' with generated response\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return {'content': 'Error loading image'}\n",
    "    \n",
    "    # Modified prompt without ground truth dependency\n",
    "    prompt = f\"\"\"You are an expert AI mathematician specializing in geometry. Your task is to analyze the geometric figure in the provided image and generate accurate geometric predicates (literals) that represent all the relationships, measurements, and properties shown in the diagram.\n",
    "\n",
    "GEOMETRY PROBLEM IMAGE:\n",
    "The image shows a geometric figure with various shapes, lines, angles, and measurements. Analyze this image carefully to understand all geometric relationships and constraints.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "YOUR TASK:\n",
    "1. First, provide step-by-step reasoning showing your analysis process\n",
    "2. Then, generate geometric predicates based on your analysis using the Guidelines below\n",
    "\n",
    "STEP-BY-STEP ANALYSIS (Required):\n",
    "Please follow this format for your reasoning:\n",
    "\n",
    "STEP 1: IMAGE ANALYSIS\n",
    "- Describe what geometric shapes, points, lines, and angles you can identify in the image\n",
    "- List all visible measurements, labels, and annotations\n",
    "- Note any special markings (right angle symbols, parallel marks, congruent marks, etc.)\n",
    "\n",
    "STEP 2: RELATIONSHIP IDENTIFICATION\n",
    "- Identify geometric relationships between elements (parallel, perpendicular, congruent, etc.)\n",
    "- Determine what constraints and properties are implied by the figure\n",
    "- Analyze how different elements connect and interact\n",
    "\n",
    "STEP 3: MEASUREMENT ANALYSIS\n",
    "- Extract all numerical values shown in the diagram\n",
    "- Identify what these measurements represent (lengths, angles, areas, etc.)\n",
    "- Determine how measurements relate to each other\n",
    "\n",
    "STEP 4: QUESTION ANALYSIS\n",
    "- Analyze what the question is asking for\n",
    "- Identify which geometric properties are relevant to solving the problem\n",
    "- Determine what information is needed to answer the question\n",
    "\n",
    "STEP 5: PREDICATE PLANNING\n",
    "- Plan which predicates are needed to represent the identified relationships\n",
    "- Ensure predicates capture all visible information and constraints\n",
    "- Consider completeness - are all important relationships captured?\n",
    "\n",
    "GUIDELINES:\n",
    "***Follow these predicates to represent diagram literals.\n",
    "\n",
    "**GEOMETRIC SHAPES:**\n",
    "- Point: Point(A), Point($)\n",
    "- Line: Line(A,B), Line(m), Line($)\n",
    "- Angle: Angle(A,B,C), Angle(A), Angle(1), Angle($)\n",
    "- Triangle: Triangle(A,B,C), Triangle($), Triangle($1,$2,$3)\n",
    "- Quadrilateral: Quadrilateral(A,B,C,D), Quadrilateral($)\n",
    "- Parallelogram: Parallelogram(A,B,C,D), Parallelogram(1), Parallelogram($)\n",
    "- Square: Square(A,B,C,D), Square(1), Square($)\n",
    "- Rectangle: Rectangle(A,B,C,D), Rectangle(1), Rectangle($)\n",
    "- Rhombus: Rhombus(A,B,C,D), Rhombus(1), Rhombus($)\n",
    "- Trapezoid: Trapezoid(A,B,C,D), Trapezoid(1), Trapezoid($)\n",
    "- Kite: Kite(A,B,C,D), Kite(1), Kite($)\n",
    "- Polygon: Polygon($)\n",
    "- Pentagon: Pentagon(A,B,C,D,E), Pentagon($)\n",
    "- Hexagon: Hexagon(A,B,C,D,E,F), Hexagon($)\n",
    "- Heptagon: Heptagon(A,B,C,D,E,F,G), Heptagon($)\n",
    "- Octagon: Octagon(A,B,C,D,E,F,G,H), Octagon($)\n",
    "- Circle: Circle(A), Circle(1), Circle($)\n",
    "- Arc: Arc(A,B), Arc(A,B,C), Arc($)\n",
    "- Sector: Sector(O,A,B), Sector($)\n",
    "- Shape: Shape($) // For unknown shapes or regions\n",
    "\n",
    "**UNARY GEOMETRIC ATTRIBUTES:**\n",
    "- RightAngle: RightAngle(Angle($))\n",
    "- Right: Right(Triangle($)) // Right triangle\n",
    "- Isosceles: Isosceles(Polygon($)) // Isosceles polygon\n",
    "- Equilateral: Equilateral(Polygon($)) // Equilateral polygon\n",
    "- Regular: Regular(Polygon($))\n",
    "- Red: Red(Shape($))\n",
    "- Blue: Blue(Shape($))\n",
    "- Green: Green(Shape($))\n",
    "- Shaded: Shaded(Shape($))\n",
    "\n",
    "**GEOMETRIC ATTRIBUTES:**\n",
    "- AreaOf: AreaOf(A)\n",
    "- PerimeterOf: PerimeterOf(A) // Perimeter of polygon A\n",
    "- RadiusOf: RadiusOf(A)\n",
    "- DiameterOf: DiameterOf(A)\n",
    "- CircumferenceOf: CircumferenceOf(A) // Perimeter of circle A\n",
    "- AltitudeOf: AltitudeOf(A) // Altitude of polygon A\n",
    "- HypotenuseOf: HypotenuseOf(A) // Hypotenuse of triangle A\n",
    "- SideOf: SideOf(A) // Side of square A\n",
    "- WidthOf: WidthOf(A) // Width of quadrilateral A\n",
    "- HeightOf: HeightOf(A) // Height of quadrilateral A\n",
    "- LegOf: LegOf(A) // Leg of trapezoid A\n",
    "- BaseOf: BaseOf(A) // Base of polygon A\n",
    "- MedianOf: MedianOf(A) // Median of polygon A\n",
    "- IntersectionOf: IntersectionOf(A,B) // Intersection of shapes A and B\n",
    "- MeasureOf: MeasureOf(A) // Measure of angle A\n",
    "- LengthOf: LengthOf(A) // Length of line A\n",
    "- ScaleFactorOf: ScaleFactorOf(A,B) // Scale factor of shape A to shape B\n",
    "\n",
    "**BINARY GEOMETRIC RELATIONS:**\n",
    "- PointLiesOnLine: PointLiesOnLine(Point($),Line($1,$2))\n",
    "- PointLiesOnCircle: PointLiesOnCircle(Point($),Circle($))\n",
    "- Parallel: Parallel(Line($),Line($))\n",
    "- Perpendicular: Perpendicular(Line($),Line($))\n",
    "- IntersectAt: IntersectAt(Line($),Line($),Line($),Point($))\n",
    "- BisectsAngle: BisectsAngle(Line($),Angle($))\n",
    "- Congruent: Congruent(Polygon($),Polygon($))\n",
    "- Similar: Similar(Polygon($),Polygon($))\n",
    "- Tangent: Tangent(Line($),Circle($))\n",
    "- Secant: Secant(Line($),Circle($))\n",
    "- CircumscribedTo: CircumscribedTo(Shape($),Shape($))\n",
    "- InscribedIn: InscribedIn(Shape($),Shape($))\n",
    "\n",
    "**A-IsXOf-B GEOMETRIC RELATIONS:**\n",
    "- IsMidpointOf: IsMidpointOf(Point($),Line($)) // Point A is midpoint of line B\n",
    "- IsCentroidOf: IsCentroidOf(Point($),Shape($)) // Point A is centroid of shape B\n",
    "- IsIncenterOf: IsIncenterOf(Point($),Shape($)) // Point A is incenter of shape B\n",
    "- IsRadiusOf: IsRadiusOf(Line($),Circle($)) // Line A is radius of circle B\n",
    "- IsDiameterOf: IsDiameterOf(Line($),Circle($)) // Line A is diameter of circle B\n",
    "- IsMidsegmentOf: IsMidsegmentOf(Line($),Triangle($)) // Line A is midsegment of triangle B\n",
    "- IsChordOf: IsChordOf(Line($),Circle($)) // Line A is chord of circle B\n",
    "- IsSideOf: IsSideOf(Line($),Polygon($)) // Line A is side of polygon B\n",
    "- IsHypotenuseOf: IsHypotenuseOf(Line($),Triangle($)) // Line A is hypotenuse of triangle B\n",
    "- IsPerpendicularBisectorOf: IsPerpendicularBisectorOf(Line($),Triangle($)) // Line A is perpendicular bisector of triangle B\n",
    "- IsAltitudeOf: IsAltitudeOf(Line($),Triangle($)) // Line A is altitude of triangle B\n",
    "- IsMedianOf: IsMedianOf(Line($),Quadrilateral($)) // Line A is median of quadrilateral B\n",
    "- IsBaseOf: IsBaseOf(Line($),Quadrilateral($)) // Line A is base of quadrilateral B\n",
    "- IsDiagonalOf: IsDiagonalOf(Line($),Quadrilateral($)) // Line A is diagonal of quadrilateral B\n",
    "- IsLegOf: IsLegOf(Line($),Trapezoid($)) // Line A is leg of trapezoid B\n",
    "\n",
    "**NUMERICAL ATTRIBUTES AND RELATIONS:**\n",
    "- SinOf: SinOf(Var)\n",
    "- CosOf: CosOf(Var)\n",
    "- TanOf: TanOf(Var)\n",
    "- CotOf: CotOf(Var)\n",
    "- HalfOf: HalfOf(Var)\n",
    "- SquareOf: SquareOf(Var)\n",
    "- SqrtOf: SqrtOf(Var)\n",
    "- RatioOf: RatioOf(Var), RatioOf(Var1,Var2)\n",
    "- SumOf: SumOf(Var1,Var2,...)\n",
    "- AverageOf: AverageOf(Var1,Var2,...)\n",
    "- Add: Add(Var1,Var2,...)\n",
    "- Mul: Mul(Var1,Var2,...)\n",
    "- Sub: Sub(Var1,Var2,...)\n",
    "- Div: Div(Var1,Var2,...)\n",
    "- Pow: Pow(Var1,Var2)\n",
    "- Equals: Equals(Var1,Var2)\n",
    "- Find: Find(Var) // Find the value of the variable\n",
    "- UseTheorem: UseTheorem(A_B_C)\n",
    "\n",
    "VARIABLE NAMING CONVENTIONS:\n",
    "- Use capital letters for points: A, B, C, D, etc.\n",
    "- Use lowercase letters for lines when not defined by points: m, n, l, etc.\n",
    "- Use numbers for unnamed shapes: 1, 2, 3, etc.\n",
    "- Use $ for generic variables: $, $1, $2, etc.\n",
    "- Use descriptive names when appropriate: base, height, radius, etc.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Carefully examine the geometric figure in the image\n",
    "2. Identify all points, lines, angles, shapes, and measurements shown\n",
    "3. **MAKE EACH PREDICATE AS ATOMIC AS POSSIBLE** \n",
    "   – Decompose any complex or compound relationship into the simplest, individual geometric \n",
    "     statements (e.g., replace \"Perpendicular(Line(A,B),Line(C,D))\" with separate vector and dot-product or angle-equals-90° predicates)\n",
    "4. Generate predicates that represent:\n",
    "   - All geometric shapes present\n",
    "   - All given measurements and their relationships\n",
    "   - All geometric properties and constraints\n",
    "   - Relationships between different elements\n",
    "5. Always provide the step-by-step reasoning first\n",
    "6. Then provide the predicates section with clear section header\n",
    "7. Follow the Guidelines above - these predicates are crucial for representing diagram literals\n",
    "8. Each predicate must be on a separate line\n",
    "9. Do not include quotation marks, extra symbols, or explanatory text in predicates\n",
    "10. Only output predicates in the exact format: PredicateName(arguments)\n",
    "11. **IMPORTANT: Do NOT include predicates that directly state the final answer or solution**\n",
    "12. **IMPORTANT: Do NOT include Find(...) predicates or any question-related predicates**\n",
    "13. Include only the given information, constraints, and geometric relationships visible in the diagram\n",
    "14. Represent all visible geometric relationships, not derived solutions\n",
    "15. The predicates should provide sufficient information for another system to solve the problem, but not the solution itself\n",
    "\n",
    "EXAMPLE OUTPUT FORMAT:\n",
    "STEP-BY-STEP REASONING:\n",
    "\n",
    "STEP 1: IMAGE ANALYSIS\n",
    "- I can see a right triangle ABC with vertices labeled A, B, C\n",
    "- Side AB is marked as 5 units\n",
    "- Side BC is marked as 12 units\n",
    "- There is a right angle symbol at vertex B\n",
    "\n",
    "STEP 2: RELATIONSHIP IDENTIFICATION\n",
    "- Triangle ABC is a right triangle with right angle at B\n",
    "- Sides AB and BC are perpendicular\n",
    "- Side AC is the hypotenuse\n",
    "\n",
    "STEP 3: MEASUREMENT ANALYSIS\n",
    "- AB = 5 units (given)\n",
    "- BC = 12 units (given)\n",
    "- AC length is what the question is asking for\n",
    "\n",
    "STEP 4: QUESTION ANALYSIS\n",
    "- The question asks for the length of AC\n",
    "- This requires using the geometric relationships visible in the triangle\n",
    "- The problem involves finding the hypotenuse of a right triangle\n",
    "\n",
    "STEP 5: PREDICATE PLANNING\n",
    "- Need to represent the triangle, right angle, measurements, and relationships\n",
    "- Must include all visible geometric constraints and properties\n",
    "\n",
    "GENERATED PREDICATES:\n",
    "Triangle(A,B,C)\n",
    "Point(A)\n",
    "Point(B)\n",
    "Point(C)\n",
    "Line(A,B)\n",
    "Line(B,C)\n",
    "Line(A,C)\n",
    "Angle(A,B,C)\n",
    "RightAngle(Angle(A,B,C))\n",
    "Right(Triangle(A,B,C))\n",
    "Equals(LengthOf(Line(A,B)), 5)\n",
    "Equals(LengthOf(Line(B,C)), 12)\n",
    "Perpendicular(Line(A,B), Line(B,C))\n",
    "IsHypotenuseOf(Line(A,C), Triangle(A,B,C))\n",
    "IsSideOf(Line(A,B), Triangle(A,B,C))\n",
    "IsSideOf(Line(B,C), Triangle(A,B,C))\n",
    "IsSideOf(Line(A,C), Triangle(A,B,C))\n",
    "Equals(SquareOf(LengthOf(Line(A,C))), Add(SquareOf(LengthOf(Line(A,B))), SquareOf(LengthOf(Line(B,C)))))\n",
    "\n",
    "Generate your complete analysis and predicates now:\n",
    "\"\"\"\n",
    "\n",
    "    # Create messages for Qwen2.5-VL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs with image\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate response with Qwen2.5-VL parameters\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4000,\n",
    "            temperature=0.8,\n",
    "            min_p=0.1,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Decode only the generated tokens (excluding input)\n",
    "    generated_tokens = outputs[:, inputs.input_ids.shape[-1]:]\n",
    "    content = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return {\n",
    "        'content': content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T12:29:36.525768Z",
     "iopub.status.busy": "2025-07-02T12:29:36.525508Z",
     "iopub.status.idle": "2025-07-02T12:29:36.553178Z",
     "shell.execute_reply": "2025-07-02T12:29:36.552392Z",
     "shell.execute_reply.started": "2025-07-02T12:29:36.525750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_reasoning_and_predicates(content):\n",
    "    \"\"\"\n",
    "    Extract reasoning and predicates sections separately from the generated content\n",
    "    \n",
    "    Args:\n",
    "        content (str): Generated content containing both reasoning and predicates\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (reasoning_content, predicates_list)\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Find the sections\n",
    "    reasoning_section = \"\"\n",
    "    predicates_section = \"\"\n",
    "    \n",
    "    reasoning_started = False\n",
    "    predicates_started = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line_upper = line.upper().strip()\n",
    "        \n",
    "        # Check for reasoning section start\n",
    "        if (\"STEP-BY-STEP REASONING\" in line_upper or \n",
    "            \"STEP 1:\" in line_upper or \n",
    "            \"REASONING\" in line_upper) and not predicates_started:\n",
    "            reasoning_started = True\n",
    "            reasoning_section += line + '\\n'\n",
    "            continue\n",
    "        \n",
    "        # Check for predicates section start\n",
    "        if (\"GENERATED PREDICATES\" in line_upper or \n",
    "            \"PREDICATES:\" in line_upper) and reasoning_started:\n",
    "            predicates_started = True\n",
    "            reasoning_started = False\n",
    "            continue\n",
    "        \n",
    "        # Add content to appropriate section\n",
    "        if reasoning_started and not predicates_started:\n",
    "            reasoning_section += line + '\\n'\n",
    "        elif predicates_started:\n",
    "            predicates_section += line + '\\n'\n",
    "    \n",
    "    # If no explicit sections found, try to extract from entire content\n",
    "    if not reasoning_section.strip() and not predicates_section.strip():\n",
    "        # Split on common patterns\n",
    "        if \"GENERATED PREDICATES\" in content.upper():\n",
    "            parts = content.upper().split(\"GENERATED PREDICATES\")\n",
    "            if len(parts) >= 2:\n",
    "                reasoning_section = parts[0]\n",
    "                predicates_section = parts[1]\n",
    "        elif \"PREDICATES:\" in content.upper():\n",
    "            parts = content.upper().split(\"PREDICATES:\")\n",
    "            if len(parts) >= 2:\n",
    "                reasoning_section = parts[0]\n",
    "                predicates_section = parts[1]\n",
    "    \n",
    "    # Use the simplified extract_predicates function\n",
    "    clean_predicates = extract_predicates(predicates_section)\n",
    "    \n",
    "    return reasoning_section.strip(), clean_predicates\n",
    "\n",
    "\n",
    "def extract_predicates(predicates_content):\n",
    "    \"\"\"\n",
    "    Extract clean predicates from the model output.\n",
    "    \n",
    "    Args:\n",
    "        predicates_content (str): The predicates section content\n",
    "        \n",
    "    Returns:\n",
    "        list: List of clean predicates\n",
    "    \"\"\"\n",
    "    lines = predicates_content.strip().split('\\n')\n",
    "    predicates = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Remove quotes and extra whitespace\n",
    "        line = line.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "        \n",
    "        # Skip lines that don't look like predicates\n",
    "        if not re.match(r'^[A-Z][a-zA-Z]*\\(.*\\)$', line):\n",
    "            continue\n",
    "            \n",
    "        predicates.append(line)\n",
    "    \n",
    "    return predicates\n",
    "\n",
    "\n",
    "def _deduplicate_predicates(predicates):\n",
    "    \"\"\"\n",
    "    Remove duplicate predicates while preserving order.\n",
    "    \n",
    "    Args:\n",
    "        predicates (list): List of predicates that may contain duplicates\n",
    "        \n",
    "    Returns:\n",
    "        list: List with duplicates removed\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    \n",
    "    for predicate in predicates:\n",
    "        # Normalize for comparison (remove extra spaces)\n",
    "        normalized = re.sub(r'\\s+', ' ', predicate.strip())\n",
    "        if normalized not in seen:\n",
    "            seen.add(normalized)\n",
    "            result.append(predicate.strip())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-02T13:36:31.054Z",
     "iopub.execute_input": "2025-07-02T12:29:36.554242Z",
     "iopub.status.busy": "2025-07-02T12:29:36.553969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Input directories\n",
    "    image_dir = \"image\"\n",
    "    questions_dir = \"question\"\n",
    "    \n",
    "    # Output directories\n",
    "    reasoning_output_dir = \"/kaggle/working/reasoning_output\"\n",
    "    predicates_output_dir = \"/kaggle/working/predicates_output\"\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(reasoning_output_dir, exist_ok=True)\n",
    "    os.makedirs(predicates_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over problem numbers 2401 to 2405 (inclusive)\n",
    "    for num in tqdm(range(2401, 3002)):\n",
    "        num_str = str(num)\n",
    "        \n",
    "        # Paths to input files\n",
    "        image_path = os.path.join(image_dir, f\"{num_str}.png\")\n",
    "        ques_path = os.path.join(questions_dir, f\"{num_str}.txt\")\n",
    "        \n",
    "        # Check if all required files exist\n",
    "        if not all(os.path.exists(path) for path in [image_path, ques_path]):\n",
    "            print(f\"Skipping problem {num_str}: Missing input files\")\n",
    "            continue\n",
    "        \n",
    "        # Read inputs\n",
    "        try:\n",
    "            with open(ques_path, \"r\") as f:\n",
    "                question = f.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading files for problem {num_str}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate predicates and reasoning\n",
    "        try:\n",
    "            result = generate_predicates_from_image(image_path, question)\n",
    "            generated_content = result['content']\n",
    "            \n",
    "            # Extract reasoning and predicates separately\n",
    "            reasoning_content, predicates_list = extract_reasoning_and_predicates(generated_content)\n",
    "            \n",
    "            # Write reasoning output to file\n",
    "            reasoning_out_path = os.path.join(reasoning_output_dir, f\"{num_str}.txt\")\n",
    "            with open(reasoning_out_path, \"w\") as f:\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(f\"STEP-BY-STEP REASONING FOR PROBLEM {num_str}\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(f\"QUESTION: {question}\\n\\n\")\n",
    "                f.write(\"REASONING:\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(reasoning_content)\n",
    "                f.write(\"\\n\" + \"=\" * 80)\n",
    "            \n",
    "            # Write clean predicates to file\n",
    "            predicates_out_path = os.path.join(predicates_output_dir, f\"{num_str}.txt\")\n",
    "            with open(predicates_out_path, \"w\") as f:\n",
    "                for pred in predicates_list:\n",
    "                    f.write(pred + \"\\n\")\n",
    "            \n",
    "            print(f\"Problem {num_str}: Generated {len(predicates_list)} predicates\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing problem {num_str}: {e}\")\n",
    "            \n",
    "            # Write error to all output files\n",
    "            error_content = f\"ERROR: {str(e)}\\n\"\n",
    "            \n",
    "            # Write error to reasoning file\n",
    "            reasoning_out_path = os.path.join(reasoning_output_dir, f\"{num_str}.txt\")\n",
    "            with open(reasoning_out_path, \"w\") as f:\n",
    "                f.write(error_content)\n",
    "            \n",
    "            # Write error to predicates file\n",
    "            predicates_out_path = os.path.join(predicates_output_dir, f\"{num_str}.txt\")\n",
    "            with open(predicates_out_path, \"w\") as f:\n",
    "                f.write(error_content)\n",
    "            \n",
    "            continue\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Outputs saved to:\")\n",
    "    print(f\"- Reasoning: {reasoning_output_dir}\")\n",
    "    print(f\"- Predicates: {predicates_output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7590724,
     "sourceId": 12305265,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
